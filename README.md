# Python_2026_mk2
cleaned up version

# üìä Portfolio Data Engineering Project ‚Äì Plan

## Stage 0 ‚Äì Baseline & Scope

- Objective: Build an end to end personal portfolio platform for **IN / CA / US** stocks, ETFs, and SGBs.
- Data source: Yahoo Finance via `yfinance` (personal, research use only).
- Special rule: **Track SGB bond performance versus gold price** (e.g., gold benchmarks or GOLDBEES), but treat BEES ETFs in general as separate; logic for them will be handled later.
- Tech stack (initial):
  - Python, Streamlit
  - pandas, yfinance
  - SQLite (later Postgres)
  - dbt, Airflow (later stages)

---

## Stage 1 ‚Äì Portfolio Upload & Viewing

**Goal:** Start from a blank Python project and get to ‚Äúupload CSV ‚Üí see portfolio‚Äù reliably.

### 1.1 Project skeleton

- Create repo structure:
  - `src/` or `app/` for code
  - `data/` for local files (CSV/JSON/DB)
  - `notebooks/` (optional) for ad hoc analysis
  - `requirements.txt` or `pyproject.toml`
- Set up virtualenv, basic lint/format (black/ruff) if desired.

### 1.2 Holdings model & CSV contract

- Define canonical holdings schema, based on `holdings-2.csv`:

  ```text
  Instrument, Qty, Avg_cost, LTP, Invested, Cur_val, PnL, Net_chg, Day_chg
‚Ä¢	Map to internal normalized model:
text
ticker, exchange, country, shares, buy_price, source, as_of_date
‚Ä¢	Add explicit handling for SGB instruments:
‚Ä¢	Recognize SGB tickers: SGB* (e.g., SGBMAY29I, SGBSEP31II-GB).
‚Ä¢	Tag them as asset_type = "SGB" and link to a gold benchmark symbol (e.g., GOLDINR, GOLDBEES) for later relative performance.
1.3 Streamlit app ‚Äì minimal version
‚Ä¢	Page: Portfolio Upload & View
‚Ä¢	st.file_uploader to upload holdings CSV.
‚Ä¢	Parse into pandas, validate:
‚Ä¢	Non empty Instrument, numeric Qty and Avg cost.
‚Ä¢	Basic country/exchange inference (initially assume NSE / India for your current file).
‚Ä¢	Persist to data/portfolio.json (or data/holdings_current.json).
‚Ä¢	Display:
‚Ä¢	Table of holdings (from JSON).
‚Ä¢	Summary metrics: total invested, current value, P&L.
1.4 Live prices integration (yfinance)
‚Ä¢	For each holding, derive the correct Yahoo symbol:
‚Ä¢	IN: append .NS or .BO as needed.
‚Ä¢	CA: append .TO / .V.
‚Ä¢	US: raw ticker.
‚Ä¢	Use yfinance to fetch:
‚Ä¢	Latest price for each ticker.
‚Ä¢	Optional: short history for charts.
‚Ä¢	Replace LTP from CSV with live quote (or show both: broker vs Yahoo).
Stage 1 Done When:
‚Ä¢	You can upload holdings-2.csv, see:
‚Ä¢	Parsed normalized holdings.
‚Ä¢	Live prices per instrument.
‚Ä¢	Basic portfolio stats in a Streamlit UI.
________________________________________
Stage 2 ‚Äì Watchlist & Research Hooks
Goal: CRUD watchlist + prep for LLM research later.
2.1 Watchlist data model
‚Ä¢	Define watchlist schema:
text
ticker, exchange, country, reason, status, added_date
‚Ä¢	Persist to data/watchlist.json.
2.2 Streamlit Watchlist UI
‚Ä¢	Add a Watchlist tab:
‚Ä¢	Add new ticker (manual form).
‚Ä¢	Promote from holdings to watchlist and vice versa.
‚Ä¢	Mark status: active / purchased / removed.
‚Ä¢	Show current price for watchlist tickers.
2.3 LLM research scaffolding
‚Ä¢	Add a panel/button:
‚Ä¢	Select ticker(s) from watchlist or holdings.
‚Ä¢	Enter a research question.
‚Ä¢	Store request metadata in data/research_requests.json (ticker, question, timestamp).
‚Ä¢	No actual LLM calls yet ‚Äì just structure hooks so later you can plug in OpenAI/other.
Stage 2 Done When:
‚Ä¢	You can maintain a watchlist in the app.
‚Ä¢	Watchlist persists across sessions.
‚Ä¢	Research requests for tickers are logged for future LLM workflows.
________________________________________
Stage 3 ‚Äì Database + Indicators + dbt
Goal: Move from flat files to a proper DB, and set up a modeling layer for indicators (RSI, ST, etc.).
3.1 Choose DB & schema
‚Ä¢	Start with SQLite (data/portfolio.db) for local dev.
‚Ä¢	Tables (raw layer):
‚Ä¢	raw_holdings
‚Ä¢	raw_watchlist
‚Ä¢	raw_price_history (daily OHLCV for all tickers)
‚Ä¢	Tables/views (modeled via dbt):
‚Ä¢	dim_ticker ‚Äì ticker metadata (ticker, exchange, country, asset_type; SGB vs equity vs ETF).
‚Ä¢	fct_positions ‚Äì position snapshots (shares, cost, market value).
‚Ä¢	fct_price_history ‚Äì cleaned OHLCV.
‚Ä¢	fct_indicators ‚Äì RSI, short/long MA, SuperTrend etc.
3.2 ETL loaders
‚Ä¢	Write Python scripts to:
‚Ä¢	Load current holdings/watchlist JSON ‚Üí raw_holdings / raw_watchlist.
‚Ä¢	For each active ticker, call yfinance and append OHLCV to raw_price_history.
3.3 dbt project
‚Ä¢	Create dbt project pointing at SQLite (or Postgres if you switch early).
‚Ä¢	Implement models:
‚Ä¢	stg_raw_holdings, stg_price_history.
‚Ä¢	fct_price_history with cleaned/standardized columns.
‚Ä¢	fct_indicators computing:
‚Ä¢	RSI (14 period).
‚Ä¢	MA (20/50/200).
‚Ä¢	SuperTrend (if desired, or leave for Python first).
‚Ä¢	Add basic dbt tests (unique keys, not null, accepted values).
3.4 SGB vs Gold tracking
‚Ä¢	In dim_ticker, tag SGB tickers and attach a gold benchmark symbol.
‚Ä¢	In fct_indicators or a dedicated fct_sgb_vs_gold model:
‚Ä¢	Compute SGB total return vs gold benchmark over multiple windows (1Y, 3Y, etc.).
‚Ä¢	This is where SGB ‚Üî gold performance logic lives (BEES ETFs can be separate later).
Stage 3 Done When:
‚Ä¢	Holdings, watchlist, and price history are in SQLite.
‚Ä¢	dbt can build indicator views.
‚Ä¢	You have at least one model that compares SGB performance vs gold.
________________________________________
Stage 4 ‚Äì Scheduling & Orchestration (Airflow-ready)
Goal: Automate daily data refresh and transformations.
4.1 ETL orchestration
‚Ä¢	Introduce Airflow locally (Docker/Astro):
‚Ä¢	DAG: portfolio_daily_etl
‚Ä¢	Task 1: ingest latest holdings (optional; if you export regularly).
‚Ä¢	Task 2: fetch daily OHLCV for all tickers (yfinance).
‚Ä¢	Task 3: run dbt run + test.
‚Ä¢	Schedule: once per day (e.g., after IN/US markets close).
4.2 Config & secrets
‚Ä¢	Move DB connection strings, API keys, etc. to:
‚Ä¢	.env for local.
‚Ä¢	Airflow connections/Variables for DAG.
4.3 Cloud ready design
‚Ä¢	Keep DB config abstract so you can change SQLite ‚Üí Postgres/Supabase later.
‚Ä¢	Optionally define a Dockerfile and docker-compose.yml for:
‚Ä¢	Streamlit
‚Ä¢	Airflow
‚Ä¢	DB (Postgres)
‚Ä¢	dbt
Stage 4 Done When:
‚Ä¢	A daily job can run unattended, updating price history and dbt models.
‚Ä¢	You can trigger the DAG manually and see updated indicators in the DB.
________________________________________
Stage 5 ‚Äì Analysis, Candlesticks & LLM Insights
Goal: Use the curated DB data for rich analytics and LLM based commentary.
5.1 Streamlit analytics pages
‚Ä¢	Pages/tabs:
‚Ä¢	Indicators & Signals:
‚Ä¢	Pull from fct_indicators.
‚Ä¢	Show RSI, MA, etc. with thresholds.
‚Ä¢	Screen tickers by signal (e.g., RSI < 30).
‚Ä¢	Candlestick & Volume Profile:
‚Ä¢	Use fct_price_history to drive Plotly candlestick/volume charts.
‚Ä¢	Reuse/refactor your existing market profile code.
‚Ä¢	SGB vs Gold:
‚Ä¢	Visual comparison of SGB returns vs gold over time.
5.2 LLM-based research & explanation
‚Ä¢	For tickers flagged by indicators (e.g., oversold SGB, high momentum equity):
‚Ä¢	Generate:
‚Ä¢	Plain English explanation of RSI/indicator state.
‚Ä¢	Optional news/summary (if you later integrate external APIs).
‚Ä¢	Persist outputs in a research_notes table:
‚Ä¢	ticker, date, signal, prompt, summary.
Stage 5 Done When:
‚Ä¢	You‚Äôre using DB/dbt data in Streamlit for:
‚Ä¢	Technical and portfolio analytics.
‚Ä¢	Visual candlestick/volume charts.
‚Ä¢	Basic LLM generated commentary.
________________________________________
Stage 6 ‚Äì Hardening & Extras (Optional)
‚Ä¢	Testing:
‚Ä¢	Unit tests for ETL and indicator logic.
‚Ä¢	dbt tests for data quality.
‚Ä¢	Data quality tooling (optional):
‚Ä¢	Great Expectations/Soda for more rigorous checks.
‚Ä¢	Extended domains:
‚Ä¢	Options positions, FX, or multi currency P&L.
‚Ä¢	Packaging:
‚Ä¢	Turn ETL/analytics into Python packages for reuse.
________________________________________
Notes & Constraints
‚Ä¢	SGB tracking: always ensure SGB performance is evaluated against a gold benchmark, not just absolute price.
‚Ä¢	BEES ETFs: treat them as normal holdings for now; add custom rules later.
‚Ä¢	Personal use: yfinance/Yahoo Finance data is for personal research; do not treat as production/commercial data.

